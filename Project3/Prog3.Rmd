---
title: "Progetto III"
author: "Simone Manti, matricola: 566908"
date: "28/12/2021"
output: pdf_document
---



```{r setup, include=FALSE}
A = read.csv("RAILFRTINTERMODAL.csv", row.names = 1, stringsAsFactors = F)
ts = ts(A, deltat=1/12, start = c(2000,1))
ts.da = decompose(ts, type = "a")
ts.dm = decompose(ts, type = "m")
ts.dar=as.vector(window(ts.da$random,c(2000,7),c(2018,6)))
ts.dmr=as.vector(window(ts.dm$random,c(2000,7),c(2018,6)))
ts.hwa = HoltWinters(ts, seasonal = "a")
ts.hwm = HoltWinters(ts, seasonal = "m")
l=length(ts)
res.hwa=rep(0,24)
res.hwm=rep(0,24)
j=1
for(i in (l-24):(l-1)){
  ts_cv=ts(ts[1:i],frequency=12,start=c(2000,1))
  ts.hwa=HoltWinters(ts_cv,seasonal="additive")
  ts.hwm=HoltWinters(ts_cv,seasonal="multiplicative")
  ts.hwa.p=predict(ts.hwa,1)
  ts.hwm.p=predict(ts.hwm,1)
  res.hwa[j]=ts.hwa.p - ts[i+1]
  res.hwm[j]=ts.hwm.p - ts[i+1]
  j=j+1
}
ts.ar = ar(ts)
ts.ls = ar(ts, method = "ols")
ts.ar.r=na.omit(ts.ar$resid)
ts.ls.r=na.omit(ts.ls$resid)
N = read.csv("Serie2.csv", row.names = 1, stringsAsFactors = F)
ts2 = ts(N, deltat=1/12, start = c(2019,1))
```

\section{Introduzione}
La seguente analisi è rivolta alle aziende dedite alla costruzione di trasporti ferroviari per merci intermodali.
Analizzeremo una serie storica (reperibile \textcolor{blue}{\href{https://fred.stlouisfed.org/series/RAILFRTINTERMODAL}{qui}}) riguardante il traffico ferroviario di merci
intermodali e il nostro obiettivo sarà predire il numero di unità nel 2019, in modo da testare la bontà e l'efficacia del metodo scelto.

\section{Presentazione del problema}

Cominciamo l’analisi osservando una rappresentazione grafica della serie storica e la sua funzione di autocorrelazione.

```{r echo=FALSE, fig.show='hold', out.height="45%", out.width="45%", fig.align='center'}
plot(ts, main = "Unità merci intermodali per trasporti ferroviari")
acf(ts, 48, main = "ACF serie storica")
```

Prima di continuare, diamo alcuni chiarimenti sulla serie storica e facciamo alcune osservazioni.

\begin{enumerate}
\item La serie storica rappresenta il numero di unità di merci intermodali trasportate per via ferroviaria per
ogni mese, dal Gennaio 2000 al Dicembre 2018.
\item Guardando l’autocorrelazione osserviamo immediatamente la presenza di un trend e anche di stagionalità,
data la presenza di "picchi" in ogni periodo.
\end{enumerate}

Nel seguito, confronteremo vari metodi per la decomposizione e l'analisi di serie storiche. Dopo aver trattato ognuno singolarmente, sceglieremo il migliore e ci concentreremo su quello per quanto riguarda la fase di predizione.

\section{Decomposizione}

Procediamo la nostra analisi guardando come si comportano i metodi di decomposizione additivo e moltiplicativo
con stagionalità fissa.



```{r echo=FALSE, fig.show='hold', out.height="45%", out.width="45%", fig.align='center'}
plot(ts.da)
plot(ts.dm)
```

Osserviamo che il metodo di decomposizione additivo sembra catturare meglio la componente stagionale, in quanto le ordinate variano in $[10^{-5}, 10^5]$. Anche i residui però variano nello stesso intervallo, proviamo a
guardare la loro autocorrelazione e il loro plot per ambedue i metodi.

```{r echo=FALSE, fig.show='hold', out.height="45%", out.width="45%", fig.align='center'}
acf(ts.dar, main = "ACF residui decomposizione additiva")
acf(ts.dmr, main = "ACF residui decomposizione moltiplicativa")
```

```{r echo=FALSE, fig.show='hold', out.height="45%", out.width="45%", fig.align='center'}
plot(ts.dar,pch = 20, main = "Residui della decomposizione additiva")
plot(ts.dmr,pch = 20, main = "Logaritmo dei residui della decomposizione moltiplicativa")
```

Effettivamente i residui hanno un ordine di grandezza pari a $10^4$, solo pochi sono dell’ordine di $10^5$. Quindi
sia nella decomposizione additiva che in quella moltiplicativa non riusciamo a catturare sufficientemente
bene la stagionalità, in quanto in ambedue i casi gli ordini di grandezza sono comparabili (in questo caso
addirittura gli stessi).


Dato che entrambe le decomposizioni a stagionalità fissa non restituiscono risultati convincenti, proviamo a
decomporre la serie storica con il metodo a stagionalità variabile.

```{r echo=FALSE, fig.show='hold', out.height="45%", out.width="45%", fig.align='center'}
ts.stl = stl(ts[,1], 7)
plot(ts.stl)
plot(ts.stl$time.series[,3], main = "Serie dei residui")
```

Ci troviamo sostanzialmente nella stessa situazione di prima, anche in questo caso non abbiamo un risultato
convincente, nel senso che non riusciamo a catturare sufficientemente bene la stagionalità. Non crediamo che
la stagionalità sia trascurabile visto anche l’aspetto della funzione di autocorrelazione della serie.

\section{Holt Winters}
Consideriamo adesso il metodo di Holt-Winters e confrontiamo il metodo di HW additivo e il metodo moltiplicativo, per capire quale dei due funziona meglio per la serie storica in esame. Riportiamo in seguito una rappresentazione grafica dei due metodi con coefficienti di default (i.e. i coefficienti ottimali ottenuti minimizzando la varianza dei residui) e i valori di tali coefficienti.

```{r echo=FALSE, fig.show='hold', out.height="45%", out.width="45%", fig.align='center'}
ts.plot(ts,ts.hwa$fitted[,1],col=c("black","red"), main = "Holt-Winters additivo")
ts.plot(ts,ts.hwm$fitted[,1],col=c("black","red"), main = "Holt-Winters moltiplicativo")
```

```{r echo=FALSE}
C = matrix(nrow = 4, ncol = 3)
C[,1] = c(" ", "alpha", "beta", "gamma")
C[1, 2] = "HW Additivo"
C[1, 3] = "HW Moltiplicativo"
C[2,2] = ts.hwa$alpha
C[2,3] = ts.hwm$alpha
C[3,2] = ts.hwa$beta
C[3,3] = ts.hwm$beta
C[4,2] = ts.hwa$gamma
C[4,3] = ts.hwm$gamma
knitr::kable(C, format="markdown")
```

Facciamo alcune osservazioni sui coefficienti.
\begin{itemize}
\item Il valore di alpha è molto grande per entrambi i metodi (circa 0.6) e quindi ambedue i metodi prediligono la componente innovativa: infatti questo si vede anche dai plot delle due serie generate, entrambe seguono molto la serie originale.
\item Il valore di beta è molto basso per entrambi i metodi (circa 0.01): questo è molto ragionevole, data la forma che ha il trend, vista nella sezione Decomposizione ("oscilla poco").
\item Il valore di gamma è molto vicino a 0.5 per entrambi i metodi (più per il metodo moltiplicativo).
\end{itemize}

Guardando i plot e le osservazioni fatte poc'anzi entrambi i metodi (sia additivo che moltiplicativo) sembrano catturare già bene la struttura della serie storica in esame, soprattutto nella parte finale (che è la più interessante per la predizione). 
Confrontiamo adesso i due metodi analizzandone in dettaglio i residui

```{r echo=FALSE, fig.show='hold', out.height="30%", out.width="30%", fig.align='center'}
ts.hwa.r=resid(ts.hwa)
ts.hwm.r=resid(ts.hwm)
vra =var(ts.hwa.r)/var(window(ts,2000))
vrm =var(ts.hwm.r)/var(window(ts,2000))
cat("Varianza non spiegata HW additivo:", vra)
cat("Varianza non spiegata HW moltiplicativo:", vrm)
plot(ts.hwa.r, type = "p", pch = 20, main = "Rappresentazione grafica residui HW-a rispetto al tempo")
plot(as.numeric(ts.hwa$fitted[,1]),as.numeric(ts.hwa.r),type="p",pch=20, main = "Rappresentazione grafica residui HW-a rispetto ai valori stimati")
acf(ts.hwa.r, main = "ACF residui HW-a")
```

```{r echo=FALSE, fig.show='hold', out.height="30%", out.width="30%", fig.align='center'}
ts.hwm.r=resid(ts.hwm)
plot(ts.hwm.r, type = "p", pch = 20, main = "Rappresentazione grafica residui HW-m rispetto al tempo")
plot(as.numeric(ts.hwm$fitted[,1]),as.numeric(ts.hwm.r),type="p",pch=20, main = "Rappresentazione grafica residui HW-m rispetto ai valori stimati")
acf(ts.hwm.r, main = "ACF residui HW-m")
```

Entrambi i metodi hanno una struttura molto simile dei residui. Inoltre, guardando le funzioni di autocorrelazione osserviamo che entrambi i metodi riescono a catturare sufficientemente bene le informazioni su trend e stagionalità della serie, poichè per ogni lag (eccetto, ovviamente, lag = 0) la ACF non è significativamente diversa da 0.
Ambedue presentano una proporzione di varianza non spiegata molto bassa (leggerissimamente meglio HW-a).

Procediamo la nostra analisi dei due metodi di HW confrontando le loro capacità predittive: per farlo valutiamo l'errore nella predizione per 2 anni tramite un procedimento di autovalutazione.   

```{r echo=FALSE, fig.show='hold', out.height="40%", out.width="40%", fig.align='center'}
cat("Varianza dell'errore nel metodo additivo:", sqrt(mean(res.hwa^2)))
cat("Varianza dell'errore nel metodo moltiplicativo:", sqrt(mean(res.hwm^2)))
train = window(ts, end = c(2016, 12))
test = window(ts, start = c(2017,1), end = c(2018,12))
tscv.hwa.p = predict(HoltWinters(train, seasonal = "a"), 24)
tscv.hwm.p = predict(HoltWinters(train, seasonal = "m"), 24)
ts.plot(test, tscv.hwa.p, tscv.hwm.p, col = c("black", "red", "blue"), main = "Previsione (24 mesi)")
legend("topleft",legend=c("HW additivo", "HW moltiplicativo"), col=c("red", "blue"), lty=1, cex=0.8)
plot(res.hwa,type="b",pch=20,col="red", main = "Errore nella previsione (24 mesi)")
legend("topright",legend=c("HW additivo", "HW moltiplicativo"), col=c("red", "blue"), lty=1, cex=0.8)
lines(res.hwm,type="b",pch=20,col="blue")
```

Possiamo fare alcune osservazioni.

\begin{enumerate}
\item Negli ultimi due anni i valori della serie sono dell'ordine di $10^6$ e per entrambi i metodi abbiamo una varianza dell'errore pari a circa $2 \times 10^4$, quindi entrambi i metodi hanno un'incertezza sulla previsione pari a circa il $2\%$.
\item Tra i due preferiamo il metodo di HW additivo, sia perchè ha una varianza dell'errore leggermente più bassa sia perchè il metodo di HW moltiplicativo ha problemi di convergenza nel dodicesimo mese di validazione (come si vede facilmente dal plot degli errori nella previsione).
\end{enumerate}

Concludiamo dalla nostra analisi che il metodo di Holt-Winters additivo si comporta meglio di quello moltiplicativo.


\section{Metodi autoregressivi}
Procediamo la nostra analisi confrontando due metodi autoregressivi: il metodo di Yule-Walker ed il metodo autoregressivo dei minimi quadrati.
Prima di farlo, guardiamo la funzione di autocorrelazione parziale della serie storica in esame.

```{r echo=FALSE, fig.show='hold', out.height="50%", out.width="50%", fig.align='center'}
pacf(ts, main = "PACF serie storica")
```

Dal plot della PACF visto poc'anzi ci aspettiamo che i metodi autoregressivi dipendano dai primi 13 istanti.
Riportiamo adesso una rappresentazione grafica dei due metodi e una tabella con i valori dei coefficienti scelti.



```{r echo=FALSE, fig.show='hold', out.height="35%", out.width="35%", fig.align='center'}
ts.plot(ts, ts - ts.ar$resid, col = c("black", "red"), main = "Metodo di Yule Walker")
ts.plot(ts, ts - ts.ls$resid, col = c("black", "blue"), main = "Metodo dei minimi quadrati")
C = matrix(nrow = 2, ncol = 15)
C[1,] = c(round(ts.ar$ar,2), NA, NA)
C[2,] = c(round(ts.ls$ar, 2))
C = data.frame(C)
rownames(C) = c("YW", "OLS")
knitr::kable(C, format="markdown")
```

Come ci aspettavamo, il metodo di YW utilizza 13 coefficienti, mentre inaspettatemente il metodo dei minimi quadrati ne utilizza 15. Osserviamo immediatamente dai plot che il metodo di YW sembra molto meno preciso (verso la fine della serie) rispetto al metodo dei minimi quadrati, che invece sembra catturare meglio la struttura della serie (soprattutto verso la fine).

Analizziamo anche in questo caso i residui per ambedue i metodi.


```{r echo=FALSE, fig.show='hold', out.height="35%", out.width="35%", fig.align='center'}
vra =var(ts.ar.r)/var(window(ts,2000))
vrm =var(ts.ls.r)/var(window(ts,2000))
cat("Varianza non spiegata Yule Walker:", vra)
cat("Varianza non spiegata Ordinary Least Squares:", vrm)
plot(ts.ar.r, type = "p", pch = 20, main = "Residui YW")
plot(ts.ls.r, type = "p", pch = 20, main = "Residui OLS")
```

```{r echo=FALSE, fig.show='hold', out.height="35%", out.width="35%", fig.align='center'}
pacf(ts.ar.r, main = "PACF residui YW")
pacf(ts.ls.r, main = "PACF residui OLS")
```

La struttura dei residui per entrambi i metodi sembra molto simile a quella dei metodi HW, anche se la PACF del metodo dei minimi quadrati presenta un picco in corrispondenza del periodo.

Confrontiamo adesso le due capacità predittive tramite un autovalidazione.

```{r echo=FALSE, fig.show='hold', out.height="35%", out.width="35%", fig.align='center'}
train = window(ts, end = c(2016, 12))
test = window(ts, start = c(2017,1), end = c(2018,12))
tscv.ar.p = predict(ar(train), n.ahead = 24, se.fit = FALSE)
tscv.ls.p = predict(ar(train, method = "ols"), n.ahead = 24, se.fit = FALSE)
ts.plot(test, tscv.ar.p, tscv.ls.p, col = c("black", "red", "blue"), main = "Previsione (24 mesi)")
legend("topleft",legend=c("Yule Walker", "Minimi quadrati"), col=c("red", "blue"), lty=1, cex=0.8)

l=length(ts)
res.arv=rep(0,24)
res.lsv=rep(0,24)
j=1
for(i in (l-24):(l-1)){
  ts_cv=ts(ts[1:i],frequency=12,start=c(2000,1))
  ts.arv=ar(ts_cv)
  ts.lsv=ar(ts_cv, method = "ols")
  ts.arv.p=predict(ts.arv,n.ahead = 1, se.fit = FALSE)
  ts.lsv.p=predict(ts.lsv,n.ahead= 1, se.fit = FALSE)
  res.arv[j]=ts.arv.p - ts[i+1]
  res.lsv[j]=ts.lsv.p - ts[i+1]
  j=j+1
}
cat(" Varianza dell'errore nel metodo Yule Walker:", sqrt(mean(res.arv^2)))
cat(" Varianza dell'errore nel metodo dei minimi quadrati:", sqrt(mean(res.lsv^2)))
plot(res.arv,type="b",pch=20,col="red", main = "Errore nella previsione (24 mesi)")
lines(res.lsv,type="b",pch=20,col="blue")
legend("topright",legend=c("Yule Walker", "Minimi quadrati"), col=c("red", "blue"), lty=1, cex=0.8)
```

Dai grafici precedenti osserviamo che non c'è paragone: Yule Walker si discosta troppo dalla serie originale e ha una varianza dell'errore decisamente più alta di quella dell'OLS. Certamente il miglior metodo tra i due è l'OLS.

\section{Scelta finale del metodo e previsione}
Scegliamo, infine, il miglior metodo tra quelli esaminati per poi procedere alla previsione del numero di unità di merci intermodali per il 2019. Il miglior metodo è il metodo dei minimi quadrati: anche se a discapito di Holt Winters additivo ha una varianza dell'errore leggermente più grande, riesce a catturare molto meglio la parte finale della serie. Infatti, questo si può vedere sia dai grafici degli errori nelle previsioni (l'errore nei mesi finali di test è molto più piccolo per OLS rispetto a HW additivo), sia dai grafici delle previsioni (la serie delle previsioni cresce leggermente per OLS nei mesi finali, mentre per HW additivo continua a scendere).
Calcoliamo l'incertezza per il metodo OLS: prima di farlo, analizziamo i residui per decidere se rigettare o meno l'ipotesi di gaussianità.


```{r echo=FALSE, fig.show='hold', out.height="35%", out.width="35%", fig.align='center'}
hist(ts.ls.r, 20, freq = F)
lines(density(ts.ls.r),col="blue")
lines(sort(ts.ls.r), dnorm(sort(ts.ls.r), mean(ts.ls.r), sd(ts.ls.r)), col = "red")
qqnorm(ts.ls.r, pch = 20)
qqline(ts.ls.r)
shapiro.test(ts.ls.r)
```

La distribuzione empirica dei residui (in blu) si discosta tanto dalla distribuzione teorica (in rosso). Inoltre, il test di SW restituisce un p-value molto basso, dunque rigettiamo decisamente l'ipotesi di gaussianità.
Possiamo, quindi, utilizzare solo incertezze empiriche. Procediamo allora con la previsione del numero di unità nel 2019. Fortunatamente, dato che siamo nel 2021, possiamo confrontare la nostra previsione con i veri risultati.

```{r echo = FALSE, out.height="60%", out.width="60%", fig.align='center'}
ts.2015 = window(ts, start = c(2017,1))
ts.ls.2015 = window(ts - ts.ls$resid, start = c(2017,1))
ts.ls.p = predict(ts.ls, n.ahead = 12, se.fit = FALSE)
ts.plot(ts.2015, ts.ls.2015, ts2, ts.ls.p, col = c("black","cyan","blue","red"))
lines(ts.ls.p+quantile(ts.ls.r,0.05),col="green3")
lines(ts.ls.p+quantile(ts.ls.r,0.95),col="green3")
legend("topleft",legend=c("Serie storica", "Minimi quadrati", "Serie nel 2019", "Previsione", "Incertezza"), col=c("black","cyan","blue","red", "green3"), lty=1, cex=0.4)
```

Il risultato della previsione è in linea con la stima dell'errore fatta prima, inoltre l'andamento della previsione è molto simile all'andamento effettivo della serie.

\section{Conclusione}
Per concludere, abbiamo confrontato diversi metodi di analisi di serie storiche per studiare la serie storica riguardante il numero di unità di merci intermodali per trasporti ferroviari. Fra tutti, il migliore è risultato il metodo autoregressivo dei minimi quadrati, il quale si è rivelato decisamente notevole in fase di previsione.














